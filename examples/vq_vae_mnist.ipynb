{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist install the library\n",
    "\n",
    "#%pip install aepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 01:07:49.589634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath('')\n",
    "sys.path.append(os.path.join(notebook_dir, '..'))\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import utils\n",
    "\n",
    "from rapidae.data.datasets import load_MNIST\n",
    "from rapidae.data.utils import evaluate, display_diff, add_noise\n",
    "from rapidae.models.vq_vae.vq_vae_model import VQ_VAE\n",
    "from rapidae.models.base.default_architectures import Encoder_Conv_VQ_MNIST, Decoder_Conv_VQ_MNIST\n",
    "from rapidae.pipelines.training import TrainingPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 01:07:52 \u001b[32m[INFO]\u001b[0m: train-images-idx3-ubyte.gz already exists.\u001b[0m\n",
      "2023-12-23 01:07:52 \u001b[32m[INFO]\u001b[0m: train-labels-idx1-ubyte.gz already exists.\u001b[0m\n",
      "2023-12-23 01:07:52 \u001b[32m[INFO]\u001b[0m: t10k-images-idx3-ubyte.gz already exists.\u001b[0m\n",
      "2023-12-23 01:07:52 \u001b[32m[INFO]\u001b[0m: t10k-labels-idx1-ubyte.gz already exists.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "x_train, y_train, x_test, y_test = load_MNIST(persistant=True)\n",
    "\n",
    "# Obtaint number of clasess\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dict(data=x_train.astype(float), labels=x_train)\n",
    "test_data = dict(data=x_test.astype(float), labels=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "model = VQ_VAE(input_dim=(x_train.shape[1], x_train.shape[2]), \n",
    "            latent_dim=2, encoder=Encoder_Conv_VQ_MNIST, decoder=Decoder_Conv_VQ_MNIST, layers_conf=[32, 64])\n",
    "\n",
    "#model.jit_compile = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-23 01:07:53 \u001b[32m[INFO]\u001b[0m: +++ training_pipeline +++\u001b[0m\n",
      "2023-12-23 01:07:53 \u001b[32m[INFO]\u001b[0m: Creating folder in ../output_dir/training_pipeline_2023-12-23_01-07-53\u001b[0m\n",
      "/home/lucas/python_envs/aepy-test/lib/python3.10/site-packages/keras/src/layers/layer.py:357: UserWarning: `build()` was called on layer 'encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/lucas/python_envs/aepy-test/lib/python3.10/site-packages/keras/src/backend/common/backend_utils.py:88: UserWarning: You might experience inconsistencies accross backends when calling conv transpose with kernel_size=3, stride=2, dilation_rate=1, padding=same, output_padding=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling VQ_VAE.call().\n\n\u001b[1mAttempt to convert a value (tensor(163.6844, grad_fn=<AddBackward0>)) with an unsupported type (<class 'torch.Tensor'>) to a Tensor.\u001b[0m\n\nArguments received by VQ_VAE.call():\n  • x=torch.Tensor(shape=torch.Size([128, 28, 28, 1]), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lucas/experiment/rapidae/examples/vq_vae_mnist.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/vq_vae_mnist.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipe \u001b[39m=\u001b[39m TrainingPipeline(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining_pipeline\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/vq_vae_mnist.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                         model\u001b[39m=\u001b[39mmodel, num_epochs\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/vq_vae_mnist.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m trained_model \u001b[39m=\u001b[39m pipe(x\u001b[39m=\u001b[39;49mx_train\u001b[39m.\u001b[39;49mastype(\u001b[39mfloat\u001b[39;49m), y\u001b[39m=\u001b[39;49my_train)\n",
      "File \u001b[0;32m~/experiment/rapidae/examples/../rapidae/pipelines/training.py:94\u001b[0m, in \u001b[0;36mTrainingPipeline.__call__\u001b[0;34m(self, x, y, x_eval, y_eval, callbacks, save_model)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, run_eagerly\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m x_eval \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     95\u001b[0m                 x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m     96\u001b[0m                 y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m     97\u001b[0m                 shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     98\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_epochs,\n\u001b[1;32m     99\u001b[0m                 batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size,\n\u001b[1;32m    100\u001b[0m                 callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    101\u001b[0m                 verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(\n\u001b[1;32m    104\u001b[0m                 x\u001b[39m=\u001b[39mx,\n\u001b[1;32m    105\u001b[0m                 y\u001b[39m=\u001b[39my,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m                 callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m    111\u001b[0m                 verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/python_envs/aepy-test/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/python_envs/aepy-test/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/python_envs/aepy-test/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/experiment/rapidae/examples/../rapidae/models/vq_vae/vq_vae_model.py:53\u001b[0m, in \u001b[0;36mVQ_VAE.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mif\u001b[39;00m vq_loss \u001b[39m==\u001b[39m []:\n\u001b[1;32m     52\u001b[0m     vq_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 53\u001b[0m vq_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(vq_loss)\n\u001b[1;32m     54\u001b[0m outputs\u001b[39m=\u001b[39m{}\n\u001b[1;32m     55\u001b[0m outputs[\u001b[39m'\u001b[39m\u001b[39mvq_loss\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m vq_loss\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling VQ_VAE.call().\n\n\u001b[1mAttempt to convert a value (tensor(163.6844, grad_fn=<AddBackward0>)) with an unsupported type (<class 'torch.Tensor'>) to a Tensor.\u001b[0m\n\nArguments received by VQ_VAE.call():\n  • x=torch.Tensor(shape=torch.Size([128, 28, 28, 1]), dtype=float32)"
     ]
    }
   ],
   "source": [
    "pipe = TrainingPipeline(name='training_pipeline',\n",
    "                        model=model, num_epochs=2)\n",
    "\n",
    "trained_model = pipe(x=x_train.astype(float), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_subplot(original, reconstructed):\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(original.squeeze() + 0.5)\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(reconstructed.squeeze() + 0.5)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(x_test), 9)\n",
    "\n",
    "test_images = x_test[idx]\n",
    "test_labels = y_test[idx]\n",
    "\n",
    "print(test_images.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "#test_data = dict(data=test_images.astype(float), labels=test_labels)\n",
    "\n",
    "reconstructions_test = trained_model.predict(test_images.astype(float))\n",
    "\n",
    "print(reconstructions_test['recon'])\n",
    "\n",
    "for test_image, reconstructed_image in zip(test_images, reconstructions_test['recon']):\n",
    "    show_subplot(test_image, reconstructed_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aepy-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

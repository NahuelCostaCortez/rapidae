{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of VQ VAE with MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First install the library\n",
    "\n",
    "# %pip install aepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Rapidae uses the new version of Keras 3, this allows the use of different backends. \n",
    "We can select among the 3 available backends (Tensorflow, Pytorch and Jax) by modifying the environment variable \"KERAS_BACKEND\".\n",
    "In the next cell we can define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rapidae'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(notebook_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m utils\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipelines\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainingPipeline\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_architectures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Encoder_Conv_VQ_MNIST, Decoder_Conv_VQ_MNIST\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrapidae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvq_vae\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvq_vae_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VQ_VAE\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rapidae'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "notebook_dir = os.path.abspath('')\n",
    "sys.path.append(os.path.join(notebook_dir, '..', 'src'))\n",
    "\n",
    "from keras import utils\n",
    "from rapidae.pipelines.training import TrainingPipeline\n",
    "from rapidae.models.base.default_architectures import Encoder_Conv_VQ_MNIST, Decoder_Conv_VQ_MNIST\n",
    "from rapidae.models.vq_vae.vq_vae_model import VQ_VAE\n",
    "from rapidae.data.utils import display_diff\n",
    "from rapidae.data.datasets import load_MNIST\n",
    "\n",
    "# For reproducibility in Keras 3. This will set:\n",
    "# 1) `numpy` seed\n",
    "# 2) backend random seed\n",
    "# 3) `python` random seed\n",
    "utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and preprocess the dataset\n",
    "\n",
    "Download and preprocess the dataset. In this example, the selected dataset is the well-known MNIST composed of handwritten number images.\n",
    "\n",
    "The \"persistant\" parameter of the load_MNIST() serves as a flag to determine if we want the dataset to be cached in the datasets folder.\n",
    "\n",
    "In this case since we are using convolutional layers we don't need to flatten the data.\n",
    "\n",
    "Train and test labels are converted into one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "x_train, y_train, x_test, y_test = load_MNIST(persistant=True)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Obtaint number of clasess\n",
    "n_classes = len(set(y_train))\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation\n",
    "\n",
    "\n",
    "Model's creation step. The selected encoder and decoder are extracted for the Keras tutorial. They are almost similar with the ones used in the vanilla vae example for MNIST, but with a few changes related to latent space, since in VQ-vae we don't have available 'z_mean' and 'z_log_var' arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "model = VQ_VAE(input_dim=(x_train.shape[1], x_train.shape[2]),\n",
    "               latent_dim=2, encoder=Encoder_Conv_VQ_MNIST, decoder=Decoder_Conv_VQ_MNIST, layers_conf=[32, 64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training pipeline \n",
    "\n",
    "Define the training pipeline. Here you can fix some hyperparameters related to the training phase of the autoencoder, like learning rate, bath size, numer of epochs, etc. \n",
    "Also you can define callbacks to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TrainingPipeline(name='training_pipeline_mnist_vq_vae',\n",
    "                        model=model, num_epochs=20)\n",
    "\n",
    "trained_model = pipe(x=x_train, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation step\n",
    "\n",
    "Let's now check the performance of this model. The original images are listed in the first row and the reconstructions in the second. These results look decent but you can play with some specific hyperparameters like the number and dimensionality of the embedding to improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions_test = trained_model.predict(x_test)\n",
    "\n",
    "display_diff(x_test, reconstructions_test['recon'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aepy-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

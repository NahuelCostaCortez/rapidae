{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fist install the library\n",
    "\n",
    "#%pip install rapidae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Rapidae uses the new version of Keras 3, this allows the use of different backends. \n",
    "We can select among the 3 available backends (Tensorflow, Pytorch and Jax) by modifying the environment variable \"KERAS_BACKEND\".\n",
    "In the next cell we can define it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/lucas/experiment/rapidae/examples/mnist_conv_vae.ipynb Cell 4\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/mnist_conv_vae.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/mnist_conv_vae.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/mnist_conv_vae.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/lucas/experiment/rapidae/examples/mnist_conv_vae.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras import utils\n",
    "from rapidae.data.datasets import load_MNIST\n",
    "from rapidae.data.utils import evaluate\n",
    "from rapidae.models.base.default_architectures import (Decoder_Conv_MNIST, Decoder_MLP,\n",
    "                                               Encoder_Conv_MNIST, Encoder_MLP)\n",
    "from rapidae.models.vae.vae_model import VAE\n",
    "from rapidae.pipelines.training import TrainingPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and preprocess the dataset. In this example, the selected dataset is the well-known MNIST composed of handwritten number images.\n",
    "\n",
    "The \"persistant\" parameter of the load_MNIST() serves as a flag to determine if we want the dataset to be cached in the datasets folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 01:20:00 \u001b[32m[INFO]\u001b[0m: train-images-idx3-ubyte.gz already exists.\u001b[0m\n",
      "2023-12-22 01:20:00 \u001b[32m[INFO]\u001b[0m: train-labels-idx1-ubyte.gz already exists.\u001b[0m\n",
      "2023-12-22 01:20:00 \u001b[32m[INFO]\u001b[0m: t10k-images-idx3-ubyte.gz already exists.\u001b[0m\n",
      "2023-12-22 01:20:00 \u001b[32m[INFO]\u001b[0m: t10k-labels-idx1-ubyte.gz already exists.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "x_train, y_train, x_test, y_test = load_MNIST(persistant=True)\n",
    "\n",
    "#x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
    "#x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255\n",
    "\n",
    "# Obtaint number of clasess\n",
    "n_classes = len(set(y_train))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert the labels into one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical\n",
    "y_train = utils.to_categorical(y_train, n_classes)\n",
    "y_test = utils.to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we are using a variational autoencoder with a classifier attached to its latent space. Weights have been included for the classifier loss as well as for the vae loss in order to increase the classifying capability of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-22 01:20:06 \u001b[32m[INFO]\u001b[0m: Classificator available for the latent space of the autoencoder\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Model creation\n",
    "model = VAE(input_dim=(x_train.shape[1], x_train.shape[2]), latent_dim=10, downstream_task='classification',\n",
    "            encoder=Encoder_Conv_MNIST, decoder=Decoder_Conv_MNIST, layers_conf=[32, 64], n_classes=n_classes,\n",
    "            weight_vae=0.25, weight_clf=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 19:45:06 \u001b[32m[INFO]\u001b[0m: +++ training_pipeline +++\u001b[0m\n",
      "2023-12-20 19:45:06 \u001b[32m[INFO]\u001b[0m: Creating folder in ../output_dir/training_pipeline_2023-12-20_19-45-06\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucaspc/venvs/keras_core/lib/python3.11/site-packages/keras/src/backend/common/backend_utils.py:88: UserWarning: You might experience inconsistencies accross backends when calling conv transpose with kernel_size=3, stride=2, dilation_rate=1, padding=same, output_padding=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: loss improved from inf to 1390968438784.00000, saving model to ../output_dir/training_pipeline_2023-12-20_19-45-06/model.weights.h5\n",
      "469/469 - 5s - 10ms/step - clf_loss: 6.9610 - kl_loss: 5563873755136.0000 - loss: 1390968438784.0000 - reconstruction_loss: 7217.3184\n",
      "Epoch 2/6\n",
      "\n",
      "Epoch 2: loss improved from 1390968438784.00000 to 79950288.00000, saving model to ../output_dir/training_pipeline_2023-12-20_19-45-06/model.weights.h5\n",
      "469/469 - 4s - 9ms/step - clf_loss: 5.1702 - kl_loss: 319793824.0000 - loss: 79950288.0000 - reconstruction_loss: 7217.2090\n",
      "Epoch 3/6\n",
      "\n",
      "Epoch 3: loss improved from 79950288.00000 to 47203956.00000, saving model to ../output_dir/training_pipeline_2023-12-20_19-45-06/model.weights.h5\n",
      "469/469 - 4s - 9ms/step - clf_loss: 4.9912 - kl_loss: 188808096.0000 - loss: 47203956.0000 - reconstruction_loss: 7217.2090\n",
      "Epoch 4/6\n",
      "\n",
      "Epoch 4: loss improved from 47203956.00000 to 29403152.00000, saving model to ../output_dir/training_pipeline_2023-12-20_19-45-06/model.weights.h5\n",
      "469/469 - 4s - 9ms/step - clf_loss: 5.1927 - kl_loss: 117604992.0000 - loss: 29403152.0000 - reconstruction_loss: 7216.8462\n",
      "Epoch 5/6\n",
      "\n",
      "Epoch 5: loss improved from 29403152.00000 to 19029530.00000, saving model to ../output_dir/training_pipeline_2023-12-20_19-45-06/model.weights.h5\n",
      "469/469 - 4s - 9ms/step - clf_loss: 4.4749 - kl_loss: 76110616.0000 - loss: 19029530.0000 - reconstruction_loss: 7217.1758\n",
      "Epoch 6/6\n",
      "\n",
      "Epoch 6: loss improved from 19029530.00000 to 13028758.00000, saving model to ../output_dir/training_pipeline_2023-12-20_19-45-06/model.weights.h5\n",
      "469/469 - 4s - 9ms/step - clf_loss: 4.1940 - kl_loss: 52107508.0000 - loss: 13028758.0000 - reconstruction_loss: 7217.0859\n"
     ]
    }
   ],
   "source": [
    "pipe = TrainingPipeline(name='training_pipeline',\n",
    "                        model=model, num_epochs=6, batch_size=128)\n",
    "\n",
    "trained_model = pipe(x=x_train.astype(float), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(filepath='../output_dir/training_pipeline_2023-11-25_00-50-30/model.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now display the label clusters of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (87367603.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def plot_label_clusters(vae,  must be equal, but ar\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_label_clusters(vae, data, labels):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = vae.encoder.predict(data, verbose=0)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_label_clusters(model, x_train, y_train)\n",
    "plot_label_clusters(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#y_train_predict = model.predict(train_data)\n",
    "y_test_predict =  model.predict(x_train)\n",
    "\n",
    "print(np.argmax(y_test_predict['clf'], axis=1))\n",
    "print(np.argmax(y_test, axis=1))\n",
    "\n",
    "target_names = ['number 0', 'number 1', 'number 2', 'number 3', 'number 4', 'number 5', 'number 6', 'number 7', 'number 8', 'number 9']\n",
    "\n",
    "#classification_report(y_train, np.argmax(y_train_predict['clf'], axis=1), target_names=target_names)\n",
    "print(classification_report(np.argmax(y_test, axis=1), np.argmax(y_test_predict['clf'], axis=1), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = trained_model.predict(x_test)\n",
    "\n",
    "evaluate(y_true=np.argmax(y_test, axis=1), \n",
    "         y_hat=np.argmax(y_hat['clf'], axis=1),\n",
    "         sel_metric=accuracy_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aepy-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
